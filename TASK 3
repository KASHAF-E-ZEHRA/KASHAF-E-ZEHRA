 TASK 3 : Create a neural network from scratch using Python and NumPy to classify handwritten digits from the MNIST dataset.

import numpy as np
import matplotlib.pyplot as plt
from keras.datasets import mnist

# Load MNIST dataset
(train_X, train_y), (test_X, test_y) = mnist.load_data()

# Normalize and flatten the images
train_X = train_X.reshape(train_X.shape[0], -1) / 255.0
test_X = test_X.reshape(test_X.shape[0], -1) / 255.0

# Convert labels to one-hot encoding
def one_hot_encode(y, num_classes):
    one_hot = np.zeros((y.size, num_classes))
    one_hot[np.arange(y.size), y] = 1
    return one_hot

train_y = one_hot_encode(train_y, 10)
test_y = one_hot_encode(test_y, 10)

# Activation functions and their derivatives
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def softmax(x):
    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_values / np.sum(exp_values, axis=1, keepdims=True)

# Neural Network class
class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.weights_input_hidden = np.random.randn(input_size, hidden_size)
        self.weights_hidden_output = np.random.randn(hidden_size, output_size)
        self.bias_hidden = np.zeros((1, hidden_size))
        self.bias_output = np.zeros((1, output_size))

    def forward(self, X):
        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden
        self.hidden_output = sigmoid(self.hidden_input)
        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output
        self.output = softmax(self.output_input)
        return self.output

    def backward(self, X, y, output, learning_rate):
        output_error = output - y
        hidden_error = np.dot(output_error, self.weights_hidden_output.T) * sigmoid_derivative(self.hidden_output)

        weights_hidden_output_gradient = np.dot(self.hidden_output.T, output_error)
        weights_input_hidden_gradient = np.dot(X.T, hidden_error)

        self.weights_hidden_output -= learning_rate * weights_hidden_output_gradient
        self.weights_input_hidden -= learning_rate * weights_input_hidden_gradient
        self.bias_output -= learning_rate * np.sum(output_error, axis=0, keepdims=True)
        self.bias_hidden -= learning_rate * np.sum(hidden_error, axis=0, keepdims=True)

    def train(self, X, y, epochs, learning_rate):
        for epoch in range(epochs):
            output = self.forward(X)
            self.backward(X, y, output, learning_rate)
            if epoch % 100 == 0:
                loss = -np.mean(np.log(output[np.arange(len(y)), np.argmax(y, axis=1)]))
                print(f'Epoch {epoch}, Loss: {loss}')

    def predict(self, X):
        output = self.forward(X)
        return np.argmax(output, axis=1)

# Training the network
input_size = 784  # 28x28 pixels
hidden_size = 64
output_size = 10
epochs = 1000
learning_rate = 0.01

nn = NeuralNetwork(input_size, hidden_size, output_size)
nn.train(train_X, train_y, epochs, learning_rate)

# Evaluate the network
predictions = nn.predict(test_X)
accuracy = np.mean(predictions == np.argmax(test_y, axis=1))
print(f'Test Accuracy: {accuracy * 100:.2f}%')

# Plotting a few predictions
def plot_image(image, label, prediction):
    plt.imshow(image.reshape(28, 28), cmap='gray')
    plt.title(f'True Label: {label}, Predicted: {prediction}')
    plt.show()

for i in range(10):
    plot_image(test_X[i], np.argmax(test_y[i]), predictions[i])
K 3 : 
